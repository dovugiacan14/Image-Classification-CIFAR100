{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[],"dockerImageVersionId":31041,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!git clone https://github.com/dovugiacan14/Image-Classification-CIFAR100","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-05-31T12:56:00.416580Z","iopub.execute_input":"2025-05-31T12:56:00.416832Z","iopub.status.idle":"2025-05-31T12:56:01.555249Z","shell.execute_reply.started":"2025-05-31T12:56:00.416814Z","shell.execute_reply":"2025-05-31T12:56:01.554347Z"}},"outputs":[{"name":"stdout","text":"Cloning into 'Image-Classification-CIFAR100'...\nremote: Enumerating objects: 119, done.\u001b[K\nremote: Counting objects: 100% (119/119), done.\u001b[K\nremote: Compressing objects: 100% (77/77), done.\u001b[K\nremote: Total 119 (delta 65), reused 86 (delta 35), pack-reused 0 (from 0)\u001b[K\nReceiving objects: 100% (119/119), 26.36 KiB | 5.27 MiB/s, done.\nResolving deltas: 100% (65/65), done.\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"!pip install torch_optimizer","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-31T12:56:01.556712Z","iopub.execute_input":"2025-05-31T12:56:01.556933Z","iopub.status.idle":"2025-05-31T12:57:14.614802Z","shell.execute_reply.started":"2025-05-31T12:56:01.556911Z","shell.execute_reply":"2025-05-31T12:57:14.613897Z"}},"outputs":[{"name":"stdout","text":"Collecting torch_optimizer\n  Downloading torch_optimizer-0.3.0-py3-none-any.whl.metadata (55 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m55.9/55.9 kB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hRequirement already satisfied: torch>=1.5.0 in /usr/local/lib/python3.11/dist-packages (from torch_optimizer) (2.6.0+cu124)\nCollecting pytorch-ranger>=0.1.1 (from torch_optimizer)\n  Downloading pytorch_ranger-0.1.1-py3-none-any.whl.metadata (509 bytes)\nRequirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch>=1.5.0->torch_optimizer) (3.18.0)\nRequirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.5.0->torch_optimizer) (4.13.2)\nRequirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=1.5.0->torch_optimizer) (3.4.2)\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.5.0->torch_optimizer) (3.1.6)\nRequirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch>=1.5.0->torch_optimizer) (2025.3.2)\nRequirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.5.0->torch_optimizer) (12.4.127)\nRequirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.5.0->torch_optimizer) (12.4.127)\nRequirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.5.0->torch_optimizer) (12.4.127)\nCollecting nvidia-cudnn-cu12==9.1.0.70 (from torch>=1.5.0->torch_optimizer)\n  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\nCollecting nvidia-cublas-cu12==12.4.5.8 (from torch>=1.5.0->torch_optimizer)\n  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-cufft-cu12==11.2.1.3 (from torch>=1.5.0->torch_optimizer)\n  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-curand-cu12==10.3.5.147 (from torch>=1.5.0->torch_optimizer)\n  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-cusolver-cu12==11.6.1.9 (from torch>=1.5.0->torch_optimizer)\n  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\nCollecting nvidia-cusparse-cu12==12.3.1.170 (from torch>=1.5.0->torch_optimizer)\n  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\nRequirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.5.0->torch_optimizer) (0.6.2)\nRequirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=1.5.0->torch_optimizer) (2.21.5)\nRequirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.5.0->torch_optimizer) (12.4.127)\nCollecting nvidia-nvjitlink-cu12==12.4.127 (from torch>=1.5.0->torch_optimizer)\n  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nRequirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.5.0->torch_optimizer) (3.2.0)\nRequirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=1.5.0->torch_optimizer) (1.13.1)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=1.5.0->torch_optimizer) (1.3.0)\nRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=1.5.0->torch_optimizer) (3.0.2)\nDownloading torch_optimizer-0.3.0-py3-none-any.whl (61 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.9/61.9 kB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading pytorch_ranger-0.1.1-py3-none-any.whl (14 kB)\nDownloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m8.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m31.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m13.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m8.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m57.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0mm\n\u001b[?25hInstalling collected packages: nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, pytorch-ranger, torch_optimizer\n  Attempting uninstall: nvidia-nvjitlink-cu12\n    Found existing installation: nvidia-nvjitlink-cu12 12.9.41\n    Uninstalling nvidia-nvjitlink-cu12-12.9.41:\n      Successfully uninstalled nvidia-nvjitlink-cu12-12.9.41\n  Attempting uninstall: nvidia-curand-cu12\n    Found existing installation: nvidia-curand-cu12 10.3.10.19\n    Uninstalling nvidia-curand-cu12-10.3.10.19:\n      Successfully uninstalled nvidia-curand-cu12-10.3.10.19\n  Attempting uninstall: nvidia-cufft-cu12\n    Found existing installation: nvidia-cufft-cu12 11.4.0.6\n    Uninstalling nvidia-cufft-cu12-11.4.0.6:\n      Successfully uninstalled nvidia-cufft-cu12-11.4.0.6\n  Attempting uninstall: nvidia-cublas-cu12\n    Found existing installation: nvidia-cublas-cu12 12.9.0.13\n    Uninstalling nvidia-cublas-cu12-12.9.0.13:\n      Successfully uninstalled nvidia-cublas-cu12-12.9.0.13\n  Attempting uninstall: nvidia-cusparse-cu12\n    Found existing installation: nvidia-cusparse-cu12 12.5.9.5\n    Uninstalling nvidia-cusparse-cu12-12.5.9.5:\n      Successfully uninstalled nvidia-cusparse-cu12-12.5.9.5\n  Attempting uninstall: nvidia-cudnn-cu12\n    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n  Attempting uninstall: nvidia-cusolver-cu12\n    Found existing installation: nvidia-cusolver-cu12 11.7.4.40\n    Uninstalling nvidia-cusolver-cu12-11.7.4.40:\n      Successfully uninstalled nvidia-cusolver-cu12-11.7.4.40\nSuccessfully installed nvidia-cublas-cu12-12.4.5.8 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127 pytorch-ranger-0.1.1 torch_optimizer-0.3.0\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"%%writefile /kaggle/working/Image-Classification-CIFAR100/config.py\nimport os\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch_optimizer import Lookahead\n\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\nsave_dir = \"checkpoints/\"\nos.makedirs(save_dir, exist_ok=True)\nnum_workers = 16\n\n\nclass CNNConfig:\n    criterion = nn.CrossEntropyLoss()\n    learning_rate = 0.001\n    num_epochs = 100\n    batch_size = 8\n    device = device\n    out_name = \"cnn_model\"\n\n    @staticmethod\n    def optimizer_fn(model):\n        return optim.Adam(model.parameters(), lr=CNNConfig.learning_rate)\n\n\nclass ResNetConfig:\n    criterion = nn.CrossEntropyLoss()\n    learning_rate = 1e-4\n    weight_decay = 1e-4\n    num_epochs = 100\n    batch_size = 32\n    device = device\n    out_name = save_dir + \"ResNet50/resnet50_model\"\n    num_workers = num_workers\n\n    @staticmethod\n    def optimizer_fn(model):\n        return optim.AdamW(\n            model.parameters(),\n            lr=ResNetConfig.learning_rate,\n            weight_decay=ResNetConfig.weight_decay,\n        )\n\n    @staticmethod\n    def scheduler_fn(optimizer):\n        return optim.lr_scheduler.CosineAnnealingLR(\n            optimizer, T_max=ResNetConfig.num_epochs\n        )\n\n\nclass VGGConfig:\n    criterion = nn.CrossEntropyLoss()\n    batch_size = 8\n    num_epochs = 100\n    learning_rate = 1e-4\n    device = device\n    out_name = \"vgg16_model\"\n\n    @staticmethod\n    def optimizer_fn(model):\n        return optim.Adam(model.parameters(), lr=VGGConfig.learning_rate)\n\n\nclass DenseNetConfig:\n    criterion = nn.CrossEntropyLoss()\n    learning_rate = 1e-4\n    weight_decay = 1e-4\n    num_epochs = 100\n    batch_size = 32\n    device = device\n    out_name = save_dir + \"DenseNet/densenet121_model\"\n\n    @staticmethod\n    def optimizer_fn(model):\n        return optim.AdamW(\n            model.parameters(),\n            lr=DenseNetConfig.learning_rate,\n            weight_decay=DenseNetConfig.weight_decay,\n        )\n\n\nclass EfficientConfig:\n    criterion = nn.CrossEntropyLoss()\n    batch_size = 32\n    num_epochs = 2\n    learning_rate = 1e-4\n    device = device\n    out_name = \"efficientnetb0_finetune_model\"\n\n    @staticmethod\n    def optimizer_fn(model):\n        return optim.Adam(model.parameters(), lr=EfficientConfig.learning_rate)\n\n\nclass ConvNeXtConfig:\n    criterion = nn.CrossEntropyLoss(label_smoothing=0.1)\n    learning_rate = 3e-4\n    num_epochs = 100\n    batch_size = 32\n    device = device\n    out_name = save_dir + \"ConvNeXt/convnext_tiny_model\"\n    grad_clip = 1.0  # gradient clipping value\n\n    @staticmethod\n    def optimizer_fn(model):\n        base_opt = optim.AdamW(\n            model.parameters(), lr=ConvNeXtConfig.learning_rate, weight_decay=1e-4\n        )\n        return Lookahead(base_opt)\n\n\nclass ViTConfig:\n    criterion = nn.CrossEntropyLoss()\n    num_epochs = 2\n    batch_size = 32\n    learning_rate = 2e-5\n    weight_decay = 0.01\n    T_max = 10\n    device = device\n    out_name = \"ViT_finetune_model\"\n\n    @staticmethod\n    def optimizer_fn(model):\n        return optim.AdamW(model.parameters(), lr=ViTConfig.learning_rate)\n\n    @staticmethod\n    def scheduler():\n        return optim.lr_scheduler.CosineAnnealingLR(\n            optimizer=ViTConfig.optimizer_fn, T_max=ViTConfig.T_max\n        )\n\n\nclass SwinConfig:\n    criterion = nn.CrossEntropyLoss()\n    learning_rate = 2e-5\n    weight_decay = 0.01\n    num_epochs = 100\n    batch_size = 32\n    device = device\n    out_name = save_dir + \"SwinTransformer/swin_transformer_tiny_model\"\n    model_name = \"microsoft/swin-tiny-patch4-window7-224\"\n\n    @staticmethod\n    def optimizer_fn(model):\n        return optim.AdamW(\n            model.parameters(),\n            lr=SwinConfig.learning_rate,\n            weight_decay=SwinConfig.weight_decay,\n        )","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-31T12:57:14.616047Z","iopub.execute_input":"2025-05-31T12:57:14.616398Z","iopub.status.idle":"2025-05-31T12:57:14.623810Z","shell.execute_reply.started":"2025-05-31T12:57:14.616368Z","shell.execute_reply":"2025-05-31T12:57:14.623187Z"}},"outputs":[{"name":"stdout","text":"Overwriting /kaggle/working/Image-Classification-CIFAR100/config.py\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"!python /kaggle/working/Image-Classification-CIFAR100/main.py --option 3","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-31T12:57:14.625156Z","iopub.execute_input":"2025-05-31T12:57:14.625457Z","execution_failed":"2025-05-31T18:18:23.619Z"}},"outputs":[{"name":"stdout","text":"2025-05-31 12:57:26.115162: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1748696246.297817      99 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1748696246.352391      99 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n100%|████████████████████████████████████████| 169M/169M [00:12<00:00, 13.7MB/s]\n----------------------------------------------------------------\n        Layer (type)               Output Shape         Param #\n================================================================\n            Conv2d-1         [-1, 64, 224, 224]           1,792\n            Conv2d-2         [-1, 64, 224, 224]          36,928\n         MaxPool2d-3         [-1, 64, 112, 112]               0\n            Conv2d-4        [-1, 128, 112, 112]          73,856\n            Conv2d-5        [-1, 128, 112, 112]         147,584\n         MaxPool2d-6          [-1, 128, 56, 56]               0\n            Conv2d-7          [-1, 256, 56, 56]         295,168\n            Conv2d-8          [-1, 256, 56, 56]         590,080\n            Conv2d-9          [-1, 256, 56, 56]         590,080\n        MaxPool2d-10          [-1, 256, 28, 28]               0\n           Conv2d-11          [-1, 512, 28, 28]       1,180,160\n           Conv2d-12          [-1, 512, 28, 28]       2,359,808\n           Conv2d-13          [-1, 512, 28, 28]       2,359,808\n        MaxPool2d-14          [-1, 512, 14, 14]               0\n           Conv2d-15          [-1, 512, 14, 14]       2,359,808\n           Conv2d-16          [-1, 512, 14, 14]       2,359,808\n           Conv2d-17          [-1, 512, 14, 14]       2,359,808\n        MaxPool2d-18            [-1, 512, 7, 7]               0\n           Linear-19                 [-1, 4096]     102,764,544\n           Linear-20                 [-1, 4096]      16,781,312\n           Linear-21                  [-1, 100]         409,700\n================================================================\nTotal params: 134,670,244\nTrainable params: 134,670,244\nNon-trainable params: 0\n----------------------------------------------------------------\nInput size (MB): 0.57\nForward/backward pass size (MB): 115.10\nParams size (MB): 513.73\nEstimated Total Size (MB): 629.40\n----------------------------------------------------------------\nEpoch [1/100], Loss: 4.6060, Train Acc: 0.94%\nNew best model saved to vgg16_model_best.pt.\nEpoch [2/100], Loss: 4.6026, Train Acc: 0.92%\nEpoch [3/100], Loss: 4.0740, Train Acc: 6.68%\nNew best model saved to vgg16_model_best.pt.\nEpoch [4/100], Loss: 3.4862, Train Acc: 16.07%\nNew best model saved to vgg16_model_best.pt.\nEpoch [5/100], Loss: 3.0748, Train Acc: 23.80%\nNew best model saved to vgg16_model_best.pt.\nEpoch [6/100], Loss: 2.7267, Train Acc: 30.85%\nNew best model saved to vgg16_model_best.pt.\nEpoch [7/100], Loss: 2.4425, Train Acc: 36.64%\nNew best model saved to vgg16_model_best.pt.\nEpoch [8/100], Loss: 2.1851, Train Acc: 42.53%\nNew best model saved to vgg16_model_best.pt.\nEpoch [9/100], Loss: 1.9317, Train Acc: 48.04%\nNew best model saved to vgg16_model_best.pt.\nEpoch [10/100], Loss: 1.6785, Train Acc: 54.18%\nNew best model saved to vgg16_model_best.pt.\nEpoch [11/100], Loss: 1.4273, Train Acc: 60.61%\nNew best model saved to vgg16_model_best.pt.\nEpoch [12/100], Loss: 1.1978, Train Acc: 66.43%\nNew best model saved to vgg16_model_best.pt.\nEpoch [13/100], Loss: 0.9871, Train Acc: 72.47%\nNew best model saved to vgg16_model_best.pt.\nEpoch [14/100], Loss: 0.8285, Train Acc: 76.72%\nNew best model saved to vgg16_model_best.pt.\nEpoch [15/100], Loss: 0.7067, Train Acc: 80.34%\nNew best model saved to vgg16_model_best.pt.\nEpoch [16/100], Loss: 0.5853, Train Acc: 83.40%\nNew best model saved to vgg16_model_best.pt.\nEpoch [17/100], Loss: 0.5172, Train Acc: 85.27%\nNew best model saved to vgg16_model_best.pt.\nEpoch [18/100], Loss: 0.4428, Train Acc: 87.37%\nNew best model saved to vgg16_model_best.pt.\nEpoch [19/100], Loss: 0.3961, Train Acc: 88.83%\nNew best model saved to vgg16_model_best.pt.\nEpoch [20/100], Loss: 0.3582, Train Acc: 89.94%\nNew best model saved to vgg16_model_best.pt.\nEpoch [21/100], Loss: 0.3201, Train Acc: 90.74%\nNew best model saved to vgg16_model_best.pt.\nEpoch [22/100], Loss: 0.2955, Train Acc: 91.44%\nNew best model saved to vgg16_model_best.pt.\nEpoch [23/100], Loss: 0.2648, Train Acc: 92.28%\nNew best model saved to vgg16_model_best.pt.\nEpoch [24/100], Loss: 0.2456, Train Acc: 92.75%\nNew best model saved to vgg16_model_best.pt.\nEpoch [25/100], Loss: 0.2280, Train Acc: 93.35%\nNew best model saved to vgg16_model_best.pt.\nEpoch [26/100], Loss: 0.2164, Train Acc: 93.49%\nNew best model saved to vgg16_model_best.pt.\nEpoch [27/100], Loss: 0.2005, Train Acc: 94.14%\nNew best model saved to vgg16_model_best.pt.\nEpoch [28/100], Loss: 0.1887, Train Acc: 94.46%\nNew best model saved to vgg16_model_best.pt.\nEpoch [29/100], Loss: 0.1794, Train Acc: 94.61%\nNew best model saved to vgg16_model_best.pt.\nEpoch [30/100], Loss: 0.1659, Train Acc: 95.09%\nNew best model saved to vgg16_model_best.pt.\nEpoch [31/100], Loss: 0.1563, Train Acc: 95.27%\nNew best model saved to vgg16_model_best.pt.\nEpoch [32/100], Loss: 0.1561, Train Acc: 95.33%\nNew best model saved to vgg16_model_best.pt.\nEpoch [33/100], Loss: 0.1434, Train Acc: 95.71%\nNew best model saved to vgg16_model_best.pt.\nEpoch [34/100], Loss: 0.1366, Train Acc: 95.95%\nNew best model saved to vgg16_model_best.pt.\nEpoch [35/100], Loss: 0.1324, Train Acc: 96.00%\nNew best model saved to vgg16_model_best.pt.\nEpoch [36/100], Loss: 0.1323, Train Acc: 96.00%\nNew best model saved to vgg16_model_best.pt.\nEpoch [37/100], Loss: 0.1199, Train Acc: 96.37%\nNew best model saved to vgg16_model_best.pt.\nEpoch [38/100], Loss: 0.1224, Train Acc: 96.33%\nEpoch [39/100], Loss: 0.1138, Train Acc: 96.57%\nNew best model saved to vgg16_model_best.pt.\nEpoch [40/100], Loss: 0.1091, Train Acc: 96.67%\nNew best model saved to vgg16_model_best.pt.\n","output_type":"stream"}],"execution_count":null}]}